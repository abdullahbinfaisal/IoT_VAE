{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdf6fa0-692b-48ce-848e-31766b876288",
   "metadata": {},
   "source": [
    "## Activation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b6699-1999-4066-a6e4-3b79068877bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Objects/typeobject.c:4243: type_traverse: Assertion failed: type_traverse() called on non-heap type 'MultiLabelSoftMarginLoss'\n",
      "Enable tracemalloc to get the memory block allocation traceback\n",
      "\n",
      "object address  : 0x5893a86eaa10\n",
      "object refcount : 2\n",
      "object type     : 0x58938d6619a0\n",
      "object type name: type\n",
      "object repr     : <class 'MultiLabelSoftMarginLoss'>\n",
      "\n",
      "Fatal Python error: _PyObject_AssertFailed: _PyObject_AssertFailed\n",
      "Python runtime state: initialized\n",
      "\n",
      "Thread 0x00007bc8757fa640 (most recent call first):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/parentpoller.py\", line 40 in run\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007bc875ffb640 (most recent call first):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 320 in wait\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/history.py\", line 894 in run\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/history.py\", line 60 in only_when_enabled\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/decorator.py\", line 232 in fun\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007bc8767fc640 (most recent call first):\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 469 in select\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871 in _run_once\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195 in start\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/control.py\", line 23 in run\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007bc876ffd640 (most recent call first):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\", line 388 in _watch_pipe_fd\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007bc8777fe640 (most recent call first):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\", line 388 in _watch_pipe_fd\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007bc8851ac640 (most recent call first):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/heartbeat.py\", line 106 in run\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Thread 0x00007bc8859ad640 (most recent call first):\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 469 in select\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871 in _run_once\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195 in start\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\", line 91 in _thread_main\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n",
      "\n",
      "Current thread 0x00007bc88973e000 (most recent call first):\n",
      "  Garbage-collecting\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pygments/style.py\", line 96 in __new__\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 455 in style_with_executing_node\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1115 in get_records\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1063 in format_exception_as_a_whole\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1173 in structured_traceback\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1326 in structured_traceback\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1435 in structured_traceback\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2142 in showtraceback\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3570 in run_code\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3488 in run_ast_nodes\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3306 in run_cell_async\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 129 in _pseudo_sync_runner\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3101 in _run_cell\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3046 in run_cell\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 549 in run_cell\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 426 in do_execute\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 758 in execute_request\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 418 in dispatch_shell\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 513 in process_one\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 524 in dispatch_queue\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80 in _run\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909 in _run_once\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195 in start\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 737 in start\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 1053 in launch_instance\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 17 in <module>\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86 in _run_code\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196 in _run_module_as_main\n",
      "\n",
      "Extension modules: zmq.backend.cython.context, zmq.backend.cython.message, zmq.backend.cython.socket, zmq.backend.cython._device, zmq.backend.cython._poll, zmq.backend.cython._proxy_steerable, zmq.backend.cython._version, zmq.backend.cython.error, zmq.backend.cython.utils, tornado.speedups, psutil._psutil_linux, psutil._psutil_posix, _pydevd_bundle.pydevd_cython, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special (total: 33)\n"
     ]
    }
   ],
   "source": [
    "from lib.CLIC_dataset import build_trainloader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from lib.student_v1 import StudentEncoderPruned_V1\n",
    "from loss import get_loss_functions\n",
    "import torch.optim as optim\n",
    "from CLIC_dataset import build_trainloader\n",
    "\n",
    "import NeuralCompression.neuralcompression.functional as ncF\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2d0fb-8626-45ad-843d-1289f8c8df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"facebookresearch/NeuralCompression\", \"msillm_quality_3\", force_reload=True)\n",
    "model = model.eval()\n",
    "model.update()\n",
    "model.update_tensor_devices(\"compress\")\n",
    "\n",
    "# Freeze Model\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99471d00-2b9e-47fc-a0b3-ad706e29bbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7e24406f5ba0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature storage for hints\n",
    "teacher_feats = {}\n",
    "\n",
    "# Assuming teacher.encoder layers accessible as .conv1, .conv2, etc.\n",
    "def get_teacher_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        teacher_feats[name] = output.detach().cpu()\n",
    "    return hook\n",
    "# Register hooks on desired hint layers\n",
    "model.encoder.blocks[0].register_forward_hook(get_teacher_hook('hint1'))\n",
    "model.encoder.blocks[1].register_forward_hook(get_teacher_hook('hint2'))\n",
    "model.encoder.blocks[2].register_forward_hook(get_teacher_hook('hint3'))\n",
    "model.encoder.blocks[3].register_forward_hook(get_teacher_hook('hint4'))\n",
    "model.encoder.blocks[4].register_forward_hook(get_teacher_hook('hint5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9565105-322b-4118-bdae-4ab1c0bbbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_trainloader(batch_size=128, img_dir=\"/workspace/unmounted/dataset/CLIC/val2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adeade-9ba7-43ee-b548-fe2c526ae000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:18<02:27,  5.90s/it]"
     ]
    }
   ],
   "source": [
    "model.encoder.to(device)\n",
    "\n",
    "for batch_idx, images in enumerate(tqdm(train_loader)):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        images, (_, _) = ncF.pad_image_to_factor(images, model._factor)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        current_batch_activations = {}\n",
    "        current_batch_activations['images'] = images.detach().cpu()\n",
    "\n",
    "        y_final = model.encoder(images)\n",
    "        current_batch_activations['layer6'] = y_final.detach().cpu()\n",
    "        \n",
    "        # Store teacher features\n",
    "        current_batch_activations['layer1'] = teacher_feats['hint1'].detach().cpu()\n",
    "        current_batch_activations['layer2'] = teacher_feats['hint2'].detach().cpu()\n",
    "        current_batch_activations['layer3'] = teacher_feats['hint3'].detach().cpu()\n",
    "        current_batch_activations['layer4'] = teacher_feats['hint4'].detach().cpu()\n",
    "        current_batch_activations['layer5'] = teacher_feats['hint5'].detach().cpu()\n",
    "\n",
    "        # Save current batch activations to separate file\n",
    "        #np.savez(f'/workspace/main/CLIC_activations/ILLM_Q3/batch_{batch_idx:04d}.npz', **current_batch_activations)\n",
    "        torch.save(current_batch_activations, f'/workspace/unmounted/CLIC_activations/ILLM_Q3_B128_torch/batch_{batch_idx:04d}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a621312b-76e7-4595-abc0-d230853c0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIC_dataset import build_activation_dataloader\n",
    "\n",
    "loader = build_activation_dataloader(npz_dir=\"/workspace/unmounted/CLIC_activations/ILLM_Q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f59289e-78aa-468e-a6f9-79dc4f9e9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from student_v1 import StudentEncoderBase_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dddaf03a-3316-4dd9-ae1c-4189c480ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 60, 224, 224])\n",
      "torch.Size([32, 120, 112, 112])\n",
      "torch.Size([32, 240, 56, 56])\n",
      "torch.Size([32, 480, 28, 28])\n",
      "torch.Size([32, 960, 14, 14])\n",
      "torch.Size([32, 220, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch['images'].shape)\n",
    "    print(batch['layer1'].shape)\n",
    "    print(batch['layer2'].shape)\n",
    "    print(batch['layer3'].shape)\n",
    "    print(batch['layer4'].shape)\n",
    "    print(batch['layer5'].shape)\n",
    "    print(batch['layer6'].shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82cad157-9dd4-4795-83a0-28a771431fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.decoder.to(device)\n",
    "student = StudentEncoderBase_V2()\n",
    "student.to(device)\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.0085)\n",
    "msssim_loss, vgg_perceptual, distillation_loss = get_loss_functions()\n",
    "vgg_perceptual = vgg_perceptual.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, epoch=None):\n",
    "    student.train()\n",
    "    running_loss = 0.0\n",
    "    total_hint1_loss = 0.0\n",
    "    total_hint2_loss = 0.0\n",
    "    total_latent_loss = 0.0\n",
    "    total_ssim_loss = 0.0\n",
    "    total_perc_loss = 0.0\n",
    "\n",
    "\n",
    "    student.to(device)\n",
    "    # Add TQDM loader\n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch if epoch is not None else ''}\")\n",
    "\n",
    "    for i, batch in loop:\n",
    "        \n",
    "        x = batch[\"images\"]\n",
    "        x = x.to(device)\n",
    "        # Padding Correction        \n",
    "        x, (_, _) = ncF.pad_image_to_factor(x, model._factor)\n",
    "         \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(\"Input Shape: \", x.shape)\n",
    "        \n",
    "        y = student.block1(x)\n",
    "        print(y.shape)\n",
    "\n",
    "        z = model.encoder.blocks(0)(x)\n",
    "\n",
    "        print(z.shape)\n",
    "        \n",
    "        print(batch['layer1'].shape)\n",
    "        hint1_loss = distillation_loss(y.to(device), batch['layer1'].to(device))\n",
    "\n",
    "        y = student.block2(y)\n",
    "        hint2_loss = distillation_loss(y.to(device), batch['layer2'].to(device))\n",
    "        \n",
    "        y = student.block3(y)\n",
    "        hint3_loss = distillation_loss(y.to(device), batch['layer3'].to(device))\n",
    "        \n",
    "        y = student.block4(y)\n",
    "        hint4_loss = distillation_loss(y.to(device), batch['layer4'].to(device))\n",
    "        \n",
    "        y = student.block5(y)\n",
    "        hint5_loss = distillation_loss(y, batch['layer5'])\n",
    "        \n",
    "        y = student.block6(y)\n",
    "        latent_loss = distillation_loss(y, batch['layer6'])\n",
    "\n",
    "        \n",
    "        #Reconstruction via Teacher Decoder\n",
    "        with torch.no_grad():            \n",
    "            x_recon = model.decoder(y)\n",
    "            #model.decoder.to(\"cpu\")\n",
    "            \n",
    "        perc_loss = vgg_perceptual(x, x_recon)\n",
    "        #ssim_loss = msssim_loss(x, x_recon)\n",
    "\n",
    "        # Cumulative Loss\n",
    "        loss = (1.0 * hint1_loss\n",
    "                + 1.0 * hint2_loss\n",
    "                + 1.0 * hint3_loss\n",
    "                + 1.0 * hint4_loss\n",
    "                + 1.0 * hint5_loss\n",
    "                + 1.0 * latent_loss\n",
    "                #+ gamma_msssim * ssim_loss\n",
    "                + gamma_perc * perc_loss)\n",
    "\n",
    "        # Backprop and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Accumulate loss values\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        total_hint1_loss += hint1_loss.item() * x.size(0)\n",
    "        total_hint2_loss += hint2_loss.item() * x.size(0)\n",
    "        total_latent_loss += latent_loss.item() * x.size(0)\n",
    "        #total_ssim_loss += ssim_loss.item() * x.size(0)\n",
    "        total_perc_loss += perc_loss.item() * x.size(0)\n",
    "\n",
    "    # Average losses\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    avg_hint1_loss = total_hint1_loss / dataset_size\n",
    "    avg_hint2_loss = total_hint2_loss / dataset_size\n",
    "    avg_latent_loss = total_latent_loss / dataset_size\n",
    "    #avg_ssim_loss = total_ssim_loss / dataset_size\n",
    "    avg_perc_loss = total_perc_loss / dataset_size\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}] Component-wise Losses:\")\n",
    "    print(f\"Hint1 Loss:     {avg_hint1_loss:.4f}\")\n",
    "    print(f\"Hint2 Loss:     {avg_hint2_loss:.4f}\")\n",
    "    print(f\"Latent Loss:    {avg_latent_loss:.4f}\")\n",
    "    #print(f\"MS-SSIM Loss:   {avg_ssim_loss:.4f}\")\n",
    "    print(f\"Perceptual Loss:{avg_perc_loss:.4f}\")\n",
    "    print(f\"Total Loss:     {epoch_loss:.4f}\")\n",
    "\n",
    "    # losses_per_epoch[] umer shall continue\n",
    "\n",
    "    # Plot reconstructed image after the epoch\n",
    "    x_vis = x[0].detach().cpu().permute(1, 2, 0)\n",
    "    x_recon_vis = x_recon[0].detach().cpu().permute(1, 2, 0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(x_vis)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(x_recon_vis)\n",
    "    axs[1].set_title(\"Reconstructed Image\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Reconstruction at Epoch {epoch}\")\n",
    "    plt.show()\n",
    "\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d9164f-17a7-4326-888f-56b62334bcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :   0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([32, 3, 256, 256])\n",
      "torch.Size([32, 60, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (int, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m y \u001b[38;5;241m=\u001b[39m student\u001b[38;5;241m.\u001b[39mblock1(x)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 41\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m(x)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (int, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "    train_loss = train_epoch(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763b819-cb3b-412e-83e1-b05c1cb5f082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
