{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5226ddd7",
   "metadata": {},
   "source": [
    "## Create a Dataset of Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779b1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa874a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherLatentSaver:\n",
    "    def __init__(self, teacher_model, dataloader, save_dir, device):\n",
    "        self.teacher = teacher_model.eval().to(device)\n",
    "        self.dataloader = dataloader\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def save_latents(self):\n",
    "        with torch.no_grad():\n",
    "            for idx, x in enumerate(self.dataloader):\n",
    "                x = x.to(self.device)\n",
    "                latent = self.teacher(x).cpu()\n",
    "                torch.save(latent, os.path.join(self.save_dir, f\"{idx:05d}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ec0504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "No of Batches = 157\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/iot/Desktop/IoT/datasets/CLIC/archive/val2017\"\n",
    "\n",
    "# 2. Create custom Dataset class\n",
    "class CLICDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split='train'):\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(self.root_dir)\n",
    "                           if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# 3. Set up transformations # LOOKUP the original PAPER's transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    RandomChoice,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "\n",
    "def default_train_transform(image_size: int) -> Compose:\n",
    "    choice_transform = RandomChoice(\n",
    "        [\n",
    "            RandomCrop(size=image_size, pad_if_needed=True, padding_mode=\"reflect\"),\n",
    "            RandomResizedCrop(size=image_size),\n",
    "        ]\n",
    "    )\n",
    "    return Compose(\n",
    "        [\n",
    "            choice_transform,\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transform = default_train_transform(224)\n",
    "\n",
    "# 4. Create datasets and dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CLICDataset(root_dir= os.path.join(path),\n",
    "                           transform=transform,\n",
    "                           split='train')\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,)\n",
    "\n",
    "# Test the dataloader\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {batch.shape}\")  # Should be [batch_size, 3, 256, 256]\n",
    "print(f\"No of Batches = {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07a4e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/iot/.cache/torch/hub/facebookresearch_NeuralCompression_main\n",
      "/home/iot/.cache/torch/hub/facebookresearch_NeuralCompression_main/neuralcompression/__init__.py:21: UserWarning: Could not retrieve neuralcompression version!\n",
      "  warnings.warn(\"Could not retrieve neuralcompression version!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HiFiCEncoder(\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): ChannelNorm2D()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ChannelNorm2D()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ChannelNorm2D()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ChannelNorm2D()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ChannelNorm2D()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load(\"facebookresearch/NeuralCompression\", \"msillm_quality_vlo1\")\n",
    "#model = model.to(device)\n",
    "model = model.eval()\n",
    "model.update()\n",
    "model.update_tensor_devices(\"compress\")\n",
    "teacher = model.encoder\n",
    "teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f50108",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.eval()\n",
    "\n",
    "saver = TeacherLatentSaver(\n",
    "    teacher_model=teacher,\n",
    "    dataloader=train_loader,\n",
    "    save_dir=\"ILLM_VLO1_Train_Latents\",  # directory to save .pt files\n",
    "    device=device\n",
    ")\n",
    "\n",
    "saver.save_latents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
